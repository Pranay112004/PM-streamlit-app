# -*- coding: utf-8 -*-
"""DSLABproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11HcjWnfwiBEeueHjsyOTL6jKGhKL32W6
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
# %matplotlib inline
import matplotlib
matplotlib.rcParams["figure.figsize"] = (20,10)

df1 = pd.read_csv('mumbai-house-price-data-cleaned.csv')
df1.head()

print(df1.isnull().sum())

print(df1.describe())


# Select only numeric columns
numeric_df = df1.select_dtypes(include=['number'])

# Now plot the heatmap
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

numerical_features = df.select_dtypes(include=['float64', 'int64']).columns

# Plot boxplots for all numerical features
for feature in numerical_features:
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=df[feature])
    plt.title(f'Boxplot of {feature}')
    plt.xlabel(feature)
    plt.show()

df1['title'].fillna('Unknown', inplace=True)

Q1 = df1['price'].quantile(0.25)
Q3 = df1['price'].quantile(0.75)
IQR = Q3 - Q1
df1 = df1[~((df1['price'] < (Q1 - 1.5 * IQR)) | (df1['price'] > (Q3 + 1.5 * IQR)))]

necessary_columns = ['price_per_sqft', 'price', 'locality', 'bedroom_num', 'area']
df1_cleaned = df1[necessary_columns].copy()

df1_cleaned['price_lakhs'] = (df1_cleaned['price'] / 100000).round(0).astype(int)

df1_cleaned = df1_cleaned.drop('price', axis=1)

def to_bhk(bedrooms):
    if pd.isna(bedrooms) or bedrooms == 0:
        return 'Unknown'
    return f'{int(bedrooms)}'
df1_cleaned['bedroom_num'] = df1_cleaned['bedroom_num'].apply(to_bhk)

print("Missing values before cleaning:")
print(df1_cleaned.isnull().sum())

df1_cleaned = df1_cleaned.dropna(subset=['price_per_sqft', 'locality', 'bedroom_num', 'area'])

df1_cleaned['locality'] = df1_cleaned['locality'].str.title()

df1_cleaned = df1_cleaned.rename(columns={
    'price_per_sqft': 'price_per_sqft',
    'locality': 'locality',
    'bedroom_num': 'bhk',
    'area': 'area_sqft'
})

print("\nFirst few rows of the cleaned dataset:")
print(df1_cleaned.head())

df1_cleaned.to_csv('mumbai-house-price-cleaned-transformed.csv', index=False)
print("\nCleaned dataset saved as 'mumbai-house-price-cleaned-transformed.csv'")

df1_cleaned['price_per_sqft'] = df1_cleaned['price_per_sqft'].round(0).astype(int)

df1_cleaned.to_csv('mumbai-house-price-cleaned-transformed.csv', index=False)
print("\nCleaned dataset saved as 'mumbai-house-price-cleaned-transformed.csv'")

print(df1_cleaned.head())

len(df1.locality.unique())

df1_cleaned.shape

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
import joblib

import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('mumbai-house-price-cleaned-transformed.csv')

# Basic pair plot
sns.pairplot(df)
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


df = pd.read_csv('mumbai-house-price-cleaned-transformed.csv')


numerical_features = df.select_dtypes(include=['float64', 'int64']).columns

for feature in numerical_features:
    plt.figure(figsize=(8, 4))
    sns.histplot(df[feature], kde=True, bins=30)
    plt.title(f'Distribution Plot for {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')
    plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import r2_score
import joblib

# Function to load and preprocess data
def load_data():
    df = pd.read_csv('mumbai-house-price-cleaned-transformed.csv')
    df = df[df['bhk'] != 'Unknown']
    df['bhk_num'] = df['bhk'].str.extract('(\d+)').astype(int)

    # Remove outliers using IQR for price_lakhs
    Q1 = df['price_lakhs'].quantile(0.25)
    Q3 = df['price_lakhs'].quantile(0.75)
    IQR = Q3 - Q1
    df = df[~((df['price_lakhs'] < (Q1 - 1.5 * IQR)) | (df['price_lakhs'] > (Q3 + 1.5 * IQR)))]

    # Cap extreme values and handle infinities
    df['price_lakhs'] = df['price_lakhs'].clip(lower=0, upper=1000)  # Cap at 1000 lakhs
    df['area_sqft'] = df['area_sqft'].clip(lower=100, upper=10000)   # Cap at 10000 sqft
    df['price_per_sqft'] = df['price_per_sqft'].clip(lower=1000, upper=100000)  # Cap at 100000/sqft
    df = df.replace([np.inf, -np.inf], np.nan).dropna()

    return df

# Function to evaluate models and return R² for price_lakhs
def evaluate_models(X, y):
    try:
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Define preprocessing
        numerical_cols = ['bhk_num', 'area_sqft', 'price_per_sqft']
        categorical_cols = ['locality']
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', StandardScaler(), numerical_cols),
                ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)
            ])

        # Define models
        models = {
            'Linear Regression': LinearRegression(),
            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
            'XGBoost': XGBRegressor(random_state=42)
        }

        # Print R² values
        print('R² Values for price_lakhs:')
        for name, model in models.items():
            pipeline = Pipeline(steps=[
                ('preprocessor', preprocessor),
                ('model', model)
            ])

            # Train
            pipeline.fit(X_train, y_train)

            # Predict and clip log-prices to avoid overflow
            y_pred = pipeline.predict(X_test)
            y_pred = np.clip(y_pred, -10, 10)  # Clip log-prices to avoid expm1 overflow

            # Convert back from log scale
            y_test_exp = np.expm1(y_test)
            y_pred_exp = np.expm1(y_pred)

            # Calculate R²
            r2 = r2_score(y_test_exp, y_pred_exp)
            print(f'{name}: {r2:.4f}')

            # Save model
            joblib.dump(pipeline, f'model_{name.lower().replace(" ", "_")}_price_lakhs.pkl')

    except Exception as e:
        print(f"Error evaluating models: {str(e)}")

# Main function
def main():
    try:
        # Load data
        df = load_data()

        # Evaluate models for price_lakhs
        X = df[['locality', 'bhk_num', 'area_sqft', 'price_per_sqft']]
        y = np.log1p(df['price_lakhs'])
        evaluate_models(X, y)

    except Exception as e:
        print(f"Error in main: {str(e)}")

if __name__ == "__main__":
    main()

len(df1_cleaned.locality.unique())

df1_cleaned.head()

df1_cleaned.locality.unique()

len(df1_cleaned.locality.unique())

df1_cleaned['locality'] = df1_cleaned['locality'].apply(lambda x: x.strip())

locality_stats = df1_cleaned.groupby('locality')['locality'].agg('count')

locality_stats = df1_cleaned['locality'].value_counts()

locality_stats

len(locality_stats[locality_stats<=10])

locality_less_than_10 = locality_stats[locality_stats<=10]

locality_less_than_10

df1_cleaned.locality = df1_cleaned.locality.apply(lambda x: 'other' if x in locality_less_than_10 else x )

len(df1_cleaned.locality.unique())

df1_cleaned.head(20)

df1_cleaned.head()

df1_cleaned.shape

df1_cleaned.isnull().sum()

df1_cleaned = pd.read_csv('mumbai-house-price-cleaned-transformed.csv')

df1_cleaned.head()

import pandas as pd

df1_cleaned = pd.read_csv('mumbai-house-price-cleaned-transformed.csv')  # replace with your actual file name

df1_cleaned.locality = df1_cleaned.locality.apply(lambda x: 'other' if x in locality_less_than_10 else x )

len(df1_cleaned.locality.unique())

df1_cleaned.head(20)

df1_cleaned.shape

df1_cleaned.price_per_sqft.describe()

df1_cleaned['price_per_sqft'] = pd.to_numeric(df1_cleaned['price_per_sqft'], errors='coerce')
df1_cleaned = df1_cleaned.dropna(subset=['price_per_sqft'])

def remove_pps_outliers(df):
    df_out = pd.DataFrame()
    for key, subdf in df.groupby('locality'):
        m = np.mean(subdf.price_per_sqft)
        st = np.std(subdf.price_per_sqft)
        # Only apply filtering if std is greater than 0
        if st != 0:
            reduced_df = subdf[(subdf.price_per_sqft > (m - st)) & (subdf.price_per_sqft <= (m + st))]
            df_out = pd.concat([df_out, reduced_df], ignore_index=True)
        else:
            df_out = pd.concat([df_out, subdf], ignore_index=True)  # keep all rows if no variation
    return df_out

df7 = remove_pps_outliers(df1_cleaned)
df7.shape

df7.shape

df8 = df7.drop(['price_per_sqft',],axis='columns')

df8.head(100)

df8.locality.unique()

dummies = pd.get_dummies(df8.locality)
dummies.head(3)

df11 = pd.concat([df8,dummies.drop('other',axis='columns')],axis='columns')
df11.head (3)

df11.shape

df12 = df11.drop(['locality',],axis='columns')

df12.head(2)

df12.shape

X = df12.drop('price_lakhs' ,axis='columns')
X.head ( )

y = df12.price_lakhs
y.head()

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load dataset
data = load_iris()
X = data.data
y = data.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Decision Tree model
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)
dt_preds = dt_model.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_preds)

# Random Forest model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_preds)

# Results
print(f"Decision Tree Accuracy: {dt_accuracy:.2f}")
print(f"Random Forest Accuracy: {rf_accuracy:.2f}")

from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_iris, load_diabetes
from sklearn.metrics import make_scorer, accuracy_score, mean_squared_error
import numpy as np

# Example for classification (Decision Tree, Random Forest)
# Load a dataset
X_classification, y_classification = load_iris(return_X_y=True)

# Initialize models
dt_model = DecisionTreeClassifier()
rf_model = RandomForestClassifier()

# Perform cross-validation
dt_scores = cross_val_score(dt_model, X_classification, y_classification, cv=5, scoring='accuracy')
rf_scores = cross_val_score(rf_model, X_classification, y_classification, cv=5, scoring='accuracy')

print("Decision Tree CV Accuracy: ", dt_scores.mean())
print("Random Forest CV Accuracy: ", rf_scores.mean())

# ---------------------------------------------------

# Example for regression (Linear Regression)
# Load a regression dataset
X_regression, y_regression = load_diabetes(return_X_y=True)

# Initialize model
lr_model = LinearRegression()

# Perform cross-validation
lr_scores = cross_val_score(lr_model, X_regression, y_regression, cv=5, scoring='neg_mean_squared_error')

# Usually, we take the absolute value for MSE
mse_scores = -lr_scores
print("Linear Regression CV Mean Squared Error: ", mse_scores.mean())

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=10)

from sklearn.linear_model import LinearRegression
lr_clf = LinearRegression()
lr_clf.fit(X_train,y_train)
lr_clf.score(X_test,y_test)

from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score

cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)
cross_val_score(LinearRegression(), X, y, cv=cv)

